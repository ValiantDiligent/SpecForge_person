#!/usr/bin/env python3
"""
åˆ†æcheckpointç»“æ„å¹¶æµ‹è¯•ç§»é™¤embeddingå±‚çš„åŠŸèƒ½
"""

import os
import torch
import json
from pathlib import Path
from collections import defaultdict
import sys

def analyze_checkpoint_structure(checkpoint_path):
    """åˆ†æcheckpointçš„ç»“æ„"""
    print(f"ğŸ” åˆ†æcheckpoint: {checkpoint_path}")
    print("=" * 80)
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {checkpoint_path}")
        return None
    
    # åŠ è½½checkpoint
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        print(f"âœ… æˆåŠŸåŠ è½½checkpoint")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None
    
    # åˆ†æcheckpointç»“æ„
    print(f"\nğŸ“Š Checkpointé¡¶å±‚é”®:")
    for key in checkpoint.keys():
        if isinstance(checkpoint[key], dict):
            print(f"  ğŸ“ {key}: dict with {len(checkpoint[key])} keys")
        elif isinstance(checkpoint[key], torch.Tensor):
            print(f"  ğŸ”¢ {key}: tensor {checkpoint[key].shape}")
        else:
            print(f"  ğŸ“„ {key}: {type(checkpoint[key])}")
    
    # å¦‚æœæœ‰modelæˆ–state_dictï¼Œåˆ†æå‚æ•°ç»“æ„
    model_dict = None
    if 'model' in checkpoint:
        model_dict = checkpoint['model']
        dict_name = 'model'
    elif 'state_dict' in checkpoint:
        model_dict = checkpoint['state_dict']
        dict_name = 'state_dict'
    else:
        # å¦‚æœcheckpointæœ¬èº«å°±æ˜¯state_dict
        if all(isinstance(v, torch.Tensor) for v in checkpoint.values()):
            model_dict = checkpoint
            dict_name = 'checkpoint'
    
    if model_dict:
        print(f"\nğŸ—ï¸ æ¨¡å‹å‚æ•°ç»“æ„ ({dict_name}):")
        analyze_model_parameters(model_dict)
    
    return checkpoint

def analyze_model_parameters(state_dict):
    """åˆ†ææ¨¡å‹å‚æ•°ç»“æ„"""
    # æŒ‰æ¨¡å—åˆ†ç»„
    modules = defaultdict(list)
    embedding_params = []
    total_params = 0
    
    for name, param in state_dict.items():
        total_params += param.numel()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯embeddingç›¸å…³å‚æ•°
        if any(embed_key in name.lower() for embed_key in ['embed', 'embedding', 'token']):
            embedding_params.append((name, param.shape, param.numel()))
        
        # æŒ‰æ¨¡å—åˆ†ç»„
        parts = name.split('.')
        if len(parts) > 1:
            module_name = parts[0]
            modules[module_name].append((name, param.shape, param.numel()))
        else:
            modules['root'].append((name, param.shape, param.numel()))
    
    # æ‰“å°æ¨¡å—ç»Ÿè®¡
    print(f"  ğŸ“ˆ æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  ğŸ“¦ æ¨¡å—æ•°é‡: {len(modules)}")
    
    print(f"\nğŸ“‹ å„æ¨¡å—å‚æ•°ç»Ÿè®¡:")
    for module_name, params in sorted(modules.items()):
        module_params = sum(p[2] for p in params)
        print(f"  ğŸ”¹ {module_name}: {len(params)} å‚æ•°, {module_params:,} å…ƒç´ ")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå‚æ•°
        for i, (name, shape, numel) in enumerate(params[:3]):
            print(f"    - {name}: {shape}")
        if len(params) > 3:
            print(f"    ... è¿˜æœ‰ {len(params) - 3} ä¸ªå‚æ•°")
    
    # ç‰¹åˆ«åˆ†æembeddingå‚æ•°
    if embedding_params:
        print(f"\nğŸ¯ Embeddingç›¸å…³å‚æ•°:")
        embedding_total = sum(p[2] for p in embedding_params)
        print(f"  ğŸ“Š Embeddingå‚æ•°æ€»æ•°: {embedding_total:,} ({embedding_total/total_params*100:.1f}%)")
        
        for name, shape, numel in embedding_params:
            print(f"  ğŸ”¸ {name}: {shape} ({numel:,} å…ƒç´ )")
    else:
        print(f"\nâŒ æœªæ‰¾åˆ°embeddingç›¸å…³å‚æ•°")

def remove_embedding_from_state_dict(state_dict, embedding_keywords=None):
    """ä»state_dictä¸­ç§»é™¤embeddingç›¸å…³å‚æ•°"""
    if embedding_keywords is None:
        embedding_keywords = ['embed', 'embedding', 'token']
    
    original_count = len(state_dict)
    original_params = sum(p.numel() for p in state_dict.values())
    
    # ä½¿ç”¨å­—å…¸æ¨å¯¼å¼ç§»é™¤embeddingå‚æ•°
    filtered_state_dict = {
        k: v for k, v in state_dict.items()
        if not any(keyword in k.lower() for keyword in embedding_keywords)
    }
    
    filtered_count = len(filtered_state_dict)
    filtered_params = sum(p.numel() for p in filtered_state_dict.values())
    
    removed_count = original_count - filtered_count
    removed_params = original_params - filtered_params
    
    print(f"\nğŸ—‘ï¸ ç§»é™¤embeddingå‚æ•°ç»“æœ:")
    print(f"  ğŸ“‰ ç§»é™¤å‚æ•°æ•°é‡: {removed_count} / {original_count}")
    print(f"  ğŸ“‰ ç§»é™¤å‚æ•°å…ƒç´ : {removed_params:,} / {original_params:,} ({removed_params/original_params*100:.1f}%)")
    
    # æ˜¾ç¤ºè¢«ç§»é™¤çš„å‚æ•°
    removed_params_list = [k for k in state_dict.keys() if k not in filtered_state_dict]
    if removed_params_list:
        print(f"  ğŸ” è¢«ç§»é™¤çš„å‚æ•°:")
        for param_name in removed_params_list:
            shape = state_dict[param_name].shape
            numel = state_dict[param_name].numel()
            print(f"    - {param_name}: {shape} ({numel:,} å…ƒç´ )")
    
    return filtered_state_dict

def test_save_and_load(original_checkpoint, filtered_state_dict, test_dir):
    """æµ‹è¯•ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½"""
    print(f"\nğŸ’¾ æµ‹è¯•ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½:")
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = Path(test_dir)
    test_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜åŸå§‹checkpointï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    original_path = test_dir / "original_checkpoint.pth"
    torch.save(original_checkpoint, original_path)
    print(f"  âœ… ä¿å­˜åŸå§‹checkpoint: {original_path}")
    
    # ä¿å­˜ç§»é™¤embeddingçš„checkpoint
    filtered_checkpoint = original_checkpoint.copy()
    
    # æ›´æ–°state_dict
    if 'model' in filtered_checkpoint:
        filtered_checkpoint['model'] = filtered_state_dict
    elif 'state_dict' in filtered_checkpoint:
        filtered_checkpoint['state_dict'] = filtered_state_dict
    else:
        filtered_checkpoint = filtered_state_dict
    
    filtered_path = test_dir / "no_embedding_checkpoint.pth"
    torch.save(filtered_checkpoint, filtered_path)
    print(f"  âœ… ä¿å­˜æ— embedding checkpoint: {filtered_path}")
    
    # æµ‹è¯•åŠ è½½
    try:
        loaded_checkpoint = torch.load(filtered_path, map_location='cpu')
        print(f"  âœ… æˆåŠŸåŠ è½½æ— embedding checkpoint")
        
        # éªŒè¯ç»“æ„
        if 'model' in loaded_checkpoint:
            loaded_state_dict = loaded_checkpoint['model']
        elif 'state_dict' in loaded_checkpoint:
            loaded_state_dict = loaded_checkpoint['state_dict']
        else:
            loaded_state_dict = loaded_checkpoint
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰embeddingå‚æ•°
        embedding_params = [k for k in loaded_state_dict.keys() 
                          if any(keyword in k.lower() for keyword in ['embed', 'embedding', 'token'])]
        
        if embedding_params:
            print(f"  âš ï¸ è­¦å‘Š: ä»ç„¶å­˜åœ¨embeddingå‚æ•°: {embedding_params}")
        else:
            print(f"  âœ… ç¡®è®¤: æ— embeddingå‚æ•°")
        
        print(f"  ğŸ“Š åŠ è½½çš„å‚æ•°æ•°é‡: {len(loaded_state_dict)}")
        print(f"  ğŸ“Š åŠ è½½çš„å‚æ•°å…ƒç´ : {sum(p.numel() for p in loaded_state_dict.values()):,}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
        return False

def create_mock_model_for_testing():
    """åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹ç”¨äºæµ‹è¯•"""
    print(f"\nğŸ§ª åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹è¿›è¡Œæµ‹è¯•:")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„transformeræ¨¡å‹state_dict
    mock_state_dict = {
        # Embeddingå±‚
        'embeddings.word_embeddings.weight': torch.randn(32000, 4096),
        'embeddings.position_embeddings.weight': torch.randn(2048, 4096),
        
        # Transformerå±‚
        'layers.0.self_attn.q_proj.weight': torch.randn(4096, 4096),
        'layers.0.self_attn.k_proj.weight': torch.randn(4096, 4096),
        'layers.0.self_attn.v_proj.weight': torch.randn(4096, 4096),
        'layers.0.self_attn.o_proj.weight': torch.randn(4096, 4096),
        'layers.0.mlp.gate_proj.weight': torch.randn(11008, 4096),
        'layers.0.mlp.up_proj.weight': torch.randn(11008, 4096),
        'layers.0.mlp.down_proj.weight': torch.randn(4096, 11008),
        'layers.0.input_layernorm.weight': torch.randn(4096),
        'layers.0.post_attention_layernorm.weight': torch.randn(4096),
        
        # è¾“å‡ºå±‚
        'lm_head.weight': torch.randn(32000, 4096),
        'norm.weight': torch.randn(4096),
    }
    
    mock_checkpoint = {
        'model': mock_state_dict,
        'optimizer': {},
        'epoch': 9,
        'global_step': 1000,
    }
    
    print(f"  âœ… åˆ›å»ºæ¨¡æ‹Ÿcheckpointï¼ŒåŒ…å« {len(mock_state_dict)} ä¸ªå‚æ•°")
    return mock_checkpoint

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Draft Model Checkpoint åˆ†æå·¥å…·")
    print("=" * 80)
    
    # ç›®æ ‡checkpointè·¯å¾„
    checkpoint_path = "/Users/zhanghuaxiang/zhx/go_learner/src/SpecForge-1/cache/dataset/Qwen3-8B-eagle3/epoch_9"
    test_dir = "./checkpoint_test"
    
    # 1. åˆ†æç°æœ‰checkpoint
    checkpoint = analyze_checkpoint_structure(checkpoint_path)
    
    if checkpoint is None:
        print(f"\nâš ï¸ æ— æ³•åŠ è½½æŒ‡å®šcheckpointï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        checkpoint = create_mock_model_for_testing()
    
    # 2. è·å–state_dict
    if 'model' in checkpoint:
        state_dict = checkpoint['model']
    elif 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    # 3. ç§»é™¤embeddingå‚æ•°
    print(f"\n" + "=" * 80)
    print(f"ğŸ”§ æµ‹è¯•ç§»é™¤embeddingåŠŸèƒ½")
    filtered_state_dict = remove_embedding_from_state_dict(state_dict)
    
    # 4. æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
    print(f"\n" + "=" * 80)
    success = test_save_and_load(checkpoint, filtered_state_dict, test_dir)
    
    # 5. æ€»ç»“
    print(f"\n" + "=" * 80)
    print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    if success:
        print(f"  âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
        print(f"  âœ… å¯ä»¥å®‰å…¨åœ°ç§»é™¤embeddingå±‚å¹¶ä¿å­˜/åŠ è½½")
        print(f"  ğŸ“ æµ‹è¯•æ–‡ä»¶ä¿å­˜åœ¨: {test_dir}")
    else:
        print(f"  âŒ æµ‹è¯•å¤±è´¥")
    
    # 6. æä¾›ä½¿ç”¨ç¤ºä¾‹
    print(f"\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ä»£ç :")
    print(f"""
# åŠ è½½checkpoint
checkpoint = torch.load('path/to/checkpoint.pth', map_location='cpu')

# ç§»é™¤embeddingå‚æ•°ï¼ˆä½¿ç”¨å­—å…¸æ¨å¯¼å¼ï¼‰
filtered_state_dict = {{
    k: v for k, v in checkpoint['model'].items()
    if not any(keyword in k.lower() for keyword in ['embed', 'embedding', 'token'])
}}

# æ›´æ–°checkpoint
checkpoint['model'] = filtered_state_dict

# ä¿å­˜æ–°çš„checkpoint
torch.save(checkpoint, 'path/to/no_embedding_checkpoint.pth')

# åŠ è½½æ—¶ä¼šè‡ªåŠ¨è·³è¿‡ç¼ºå¤±çš„embeddingå‚æ•°
model.load_state_dict(filtered_state_dict, strict=False)
""")

if __name__ == "__main__":
    main()