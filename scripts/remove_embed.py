#!/usr/bin/env python3
"""
åˆ†æå’Œå¤„ç†draft modelæƒé‡çš„è„šæœ¬
åŠŸèƒ½ï¼š
1. æŸ¥çœ‹æƒé‡ç»“æ„
2. ç§»é™¤embeddingå±‚
3. æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
4. æ”¯æŒsafetensoræ ¼å¼
"""

import os
import torch
import json
from pathlib import Path
from collections import OrderedDict
import sys
import argparse
from specforge import (
    AutoEagle3DraftModel,
)

# å°è¯•å¯¼å…¥safetensors
try:
    from safetensors import safe_open
    from safetensors.torch import save_file as safe_save_file
    SAFETENSORS_AVAILABLE = True
except ImportError:
    print("âš ï¸ safetensors not available, will use pytorch format")
    SAFETENSORS_AVAILABLE = False


def detect_model_format(checkpoint_dir):
    """
    æ£€æµ‹æ¨¡å‹æ–‡ä»¶æ ¼å¼
    
    Returns:
        str: 'safetensors', 'pytorch', or 'unknown'
    """
    safetensor_path = os.path.join(checkpoint_dir, "model.safetensors")
    pytorch_path = os.path.join(checkpoint_dir, "pytorch_model.bin")
    
    if os.path.exists(safetensor_path):
        return 'safetensors'
    elif os.path.exists(pytorch_path):
        return 'pytorch'
    else:
        return 'unknown'


def load_state_dict_from_checkpoint(checkpoint_dir):
    """
    ä»æ£€æŸ¥ç‚¹ç›®å½•åŠ è½½state dictï¼Œè‡ªåŠ¨æ£€æµ‹æ ¼å¼
    
    Args:
        checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
        
    Returns:
        tuple: (state_dict, format_type)
    """
    format_type = detect_model_format(checkpoint_dir)
    
    if format_type == 'safetensors' and SAFETENSORS_AVAILABLE:
        safetensor_path = os.path.join(checkpoint_dir, "model.safetensors")
        print(f"ğŸ“‚ åŠ è½½safetensoræ ¼å¼: {safetensor_path}")
        
        state_dict = {}
        with safe_open(safetensor_path, framework="pt", device="cpu") as f:
            for key in f.keys():
                state_dict[key] = f.get_tensor(key)
        
        return state_dict, 'safetensors'
        
    elif format_type == 'pytorch':
        pytorch_path = os.path.join(checkpoint_dir, "pytorch_model.bin")
        print(f"ğŸ“‚ åŠ è½½pytorchæ ¼å¼: {pytorch_path}")
        
        state_dict = torch.load(pytorch_path, map_location="cpu", weights_only=True)
        return state_dict, 'pytorch'
        
    else:
        raise ValueError(f"æ— æ³•æ‰¾åˆ°æ”¯æŒçš„æ¨¡å‹æ–‡ä»¶æ ¼å¼åœ¨ {checkpoint_dir}")


def save_state_dict_to_checkpoint(state_dict, output_dir, format_type='safetensors'):
    """
    ä¿å­˜state dictåˆ°æ£€æŸ¥ç‚¹ç›®å½•
    
    Args:
        state_dict: è¦ä¿å­˜çš„state dict
        output_dir: è¾“å‡ºç›®å½•
        format_type: ä¿å­˜æ ¼å¼ ('safetensors' æˆ– 'pytorch')
    """
    os.makedirs(output_dir, exist_ok=True)
    
    if format_type == 'safetensors' and SAFETENSORS_AVAILABLE:
        safetensor_path = os.path.join(output_dir, "model.safetensors")
        print(f"ğŸ’¾ ä¿å­˜ä¸ºsafetensoræ ¼å¼: {safetensor_path}")
        
        # ç¡®ä¿æ‰€æœ‰tensoréƒ½åœ¨CPUä¸Š
        cpu_state_dict = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in state_dict.items()}
        safe_save_file(cpu_state_dict, safetensor_path)
        
        return safetensor_path
        
    else:
        pytorch_path = os.path.join(output_dir, "pytorch_model.bin")
        print(f"ğŸ’¾ ä¿å­˜ä¸ºpytorchæ ¼å¼: {pytorch_path}")
        
        torch.save(state_dict, pytorch_path)
        return pytorch_path


def print_model_info(model, title="æ¨¡å‹ä¿¡æ¯"):
    """æ‰“å°æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
    print(f"\n{'='*20} {title} {'='*20}")
    print("æ¨¡å‹ç»“æ„ï¼š")
    print(model)
    print(f"\næ¨¡å‹æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M")
    print("=" * 70)
    print("æ‰€æœ‰å‚æ•°:")
    for name, param in model.named_parameters():
        print(f"{name}: {tuple(param.shape)} - {'trainable' if param.requires_grad else 'frozen'}")
    print("=" * 70)


def analyze_state_dict(state_dict, title="State Dictåˆ†æ"):
    """åˆ†æstate dictç»“æ„"""
    print(f"\n{'='*20} {title} {'='*20}")
    total_params = 0
    embed_params = 0
    
    for key, tensor in state_dict.items():
        param_count = tensor.numel()
        total_params += param_count
        
        if "embed" in key.lower():
            embed_params += param_count
            print(f"ğŸ” EMBEDDING: {key}: {tuple(tensor.shape)} ({param_count:,} params)")
        else:
            print(f"   {key}: {tuple(tensor.shape)} ({param_count:,} params)")
    
    print(f"\nğŸ“Š æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    print(f"ğŸ“Š Embeddingå‚æ•°é‡: {embed_params:,} ({embed_params/1e6:.2f}M)")
    print(f"ğŸ“Š éEmbeddingå‚æ•°é‡: {(total_params-embed_params):,} ({(total_params-embed_params)/1e6:.2f}M)")
    print("=" * 70)


def remove_embedding_from_state_dict(state_dict, embedding_keys=None):
    """
    ä»state dictä¸­ç§»é™¤embeddingç›¸å…³çš„æƒé‡
    
    Args:
        state_dict: åŸå§‹state dict
        embedding_keys: è¦ç§»é™¤çš„embedding keyåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
    
    Returns:
        æ–°çš„state dictï¼ˆä¸åŒ…å«embeddingï¼‰
    """
    if embedding_keys is None:
        # è‡ªåŠ¨æ£€æµ‹embeddingç›¸å…³çš„key
        embedding_keys = [
            "embed_tokens.weight",
            "model.embed_tokens.weight", 
            "embeddings.word_embeddings.weight",
            "word_embeddings.weight"
        ]
    
    new_state_dict = OrderedDict()
    removed_keys = []
    
    for key, value in state_dict.items():
        # æ£€æŸ¥æ˜¯å¦æ˜¯embeddingç›¸å…³çš„key
        should_remove = False
        for embed_key in embedding_keys:
            if embed_key in key or "embed" in key.lower():
                should_remove = True
                removed_keys.append(key)
                break
        
        if not should_remove:
            new_state_dict[key] = value
    
    print(f"\nğŸ—‘ï¸ ç§»é™¤çš„embeddingå±‚:")
    for key in removed_keys:
        print(f"   - {key}")
    
    return new_state_dict, removed_keys


def copy_other_files(input_dir, output_dir):
    """
    å¤åˆ¶å…¶ä»–å¿…è¦çš„æ–‡ä»¶ï¼ˆconfig.json, training_state.ptç­‰ï¼‰
    """
    files_to_copy = [
        "config.json",
        "training_state.pt",
        "generation_config.json",
        "tokenizer.json",
        "tokenizer_config.json",
        "special_tokens_map.json",
        "vocab.txt"
    ]
    
    copied_files = []
    for filename in files_to_copy:
        src_path = os.path.join(input_dir, filename)
        dst_path = os.path.join(output_dir, filename)
        
        if os.path.exists(src_path):
            import shutil
            shutil.copy2(src_path, dst_path)
            copied_files.append(filename)
            print(f"ğŸ“‹ å¤åˆ¶æ–‡ä»¶: {filename}")
    
    return copied_files


def process_checkpoint_directly(input_dir, output_dir, embedding_keys=None, preserve_format=True):
    """
    ç›´æ¥å¤„ç†æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œç§»é™¤embeddingå±‚
    
    Args:
        input_dir: è¾“å…¥æ£€æŸ¥ç‚¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        embedding_keys: è¦ç§»é™¤çš„embedding keyåˆ—è¡¨
        preserve_format: æ˜¯å¦ä¿æŒåŸå§‹æ ¼å¼
    """
    print(f"\nğŸ”„ ç›´æ¥å¤„ç†æ£€æŸ¥ç‚¹æ–‡ä»¶")
    
    # æ£€æµ‹è¾“å…¥æ ¼å¼
    input_format = detect_model_format(input_dir)
    print(f"ğŸ“‚ æ£€æµ‹åˆ°è¾“å…¥æ ¼å¼: {input_format}")
    
    if input_format == 'unknown':
        raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ¨¡å‹æ ¼å¼åœ¨ {input_dir}")
    
    # åŠ è½½state dict
    state_dict, detected_format = load_state_dict_from_checkpoint(input_dir)
    print(f"ğŸ“‹ åŠ è½½äº† {len(state_dict)} ä¸ªå‚æ•°")
    
    # åˆ†æåŸå§‹state dict
    analyze_state_dict(state_dict, "åŸå§‹State Dict")
    
    # ç§»é™¤embedding
    new_state_dict, removed_keys = remove_embedding_from_state_dict(
        state_dict, embedding_keys
    )
    
    # åˆ†ææ–°çš„state dict
    analyze_state_dict(new_state_dict, "ç§»é™¤Embeddingåçš„State Dict")
    
    # ç¡®å®šè¾“å‡ºæ ¼å¼
    output_format = detected_format if preserve_format else 'safetensors'
    if not SAFETENSORS_AVAILABLE and output_format == 'safetensors':
        output_format = 'pytorch'
        print("âš ï¸ safetensorsä¸å¯ç”¨ï¼Œä½¿ç”¨pytorchæ ¼å¼")
    
    # ä¿å­˜æ–°çš„state dict
    saved_path = save_state_dict_to_checkpoint(new_state_dict, output_dir, output_format)
    print(f"âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜: {saved_path}")
    
    # å¤åˆ¶å…¶ä»–æ–‡ä»¶
    copied_files = copy_other_files(input_dir, output_dir)
    
    # ä¿å­˜ç§»é™¤ä¿¡æ¯
    removed_info = {
        "removed_keys": removed_keys,
        "original_param_count": len(state_dict),
        "new_param_count": len(new_state_dict),
        "removed_param_count": len(removed_keys),
        "input_format": detected_format,
        "output_format": output_format,
        "copied_files": copied_files
    }
    
    info_path = os.path.join(output_dir, "removed_embedding_info.json")
    with open(info_path, 'w') as f:
        json.dump(removed_info, f, indent=2)
    print(f"âœ… ç§»é™¤ä¿¡æ¯å·²ä¿å­˜: {info_path}")
    
    return new_state_dict, removed_keys


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç§»é™¤draft modelçš„embeddingå±‚")
    parser.add_argument("--input-dir", type=str, required=True,
                       help="è¾“å…¥æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•")
    parser.add_argument("--output-dir", type=str, required=True,
                       help="è¾“å‡ºç›®å½•ï¼ˆä¿å­˜ç§»é™¤embeddingåçš„æ¨¡å‹ï¼‰")
    parser.add_argument("--embedding-keys", type=str, nargs="+", 
                       default=["embed_tokens.weight"],
                       help="è¦ç§»é™¤çš„embedding keyåˆ—è¡¨")
    parser.add_argument("--analyze-only", action="store_true",
                       help="åªåˆ†ææ¨¡å‹ï¼Œä¸ä¿å­˜")
    parser.add_argument("--direct-process", action="store_true",
                       help="ç›´æ¥å¤„ç†æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆä¸é€šè¿‡æ¨¡å‹åŠ è½½ï¼‰")
    parser.add_argument("--output-format", type=str, choices=['safetensors', 'pytorch', 'auto'],
                       default='auto', help="è¾“å‡ºæ ¼å¼")
    
    args = parser.parse_args()
    
    print("ğŸš€ Draft Model Embeddingç§»é™¤å·¥å…·")
    print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ¯ è¦ç§»é™¤çš„embedding keys: {args.embedding_keys}")
    print(f"ğŸ”§ Safetensorsæ”¯æŒ: {'âœ…' if SAFETENSORS_AVAILABLE else 'âŒ'}")
    
    if not os.path.exists(args.input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    try:
        if args.analyze_only:
            # åªåˆ†ææ¨¡å‹
            print(f"\nğŸ” åˆ†ææ¨¡å¼ï¼šåªæŸ¥çœ‹æ¨¡å‹ç»“æ„")
            
            if args.direct_process:
                # ç›´æ¥åˆ†ææ–‡ä»¶
                state_dict, format_type = load_state_dict_from_checkpoint(args.input_dir)
                analyze_state_dict(state_dict, f"State Dictåˆ†æ ({format_type})")
            else:
                # é€šè¿‡æ¨¡å‹åŠ è½½åˆ†æ
                draft_model = AutoEagle3DraftModel.from_pretrained(args.input_dir).to(torch.bfloat16)
                print_model_info(draft_model, "æ¨¡å‹åˆ†æ")
                analyze_state_dict(draft_model.state_dict(), "State Dictåˆ†æ")
        else:
            # å¤„ç†å’Œä¿å­˜
            if args.direct_process:
                # ç›´æ¥å¤„ç†æ–‡ä»¶
                preserve_format = (args.output_format == 'auto')
                new_state_dict, removed_keys = process_checkpoint_directly(
                    args.input_dir, args.output_dir, args.embedding_keys, preserve_format
                )
            else:
                # é€šè¿‡æ¨¡å‹åŠ è½½å¤„ç†
                print("âš ï¸ ä½¿ç”¨æ¨¡å‹åŠ è½½æ–¹å¼ï¼Œå°†ä¿å­˜ä¸ºpytorchæ ¼å¼")
                draft_model = AutoEagle3DraftModel.from_pretrained(args.input_dir).to(torch.bfloat16)
                print_model_info(draft_model, "åŸå§‹æ¨¡å‹")
                analyze_state_dict(draft_model.state_dict(), "åŸå§‹State Dict")
                
                # ç§»é™¤embeddingå¹¶ä¿å­˜
                original_state_dict = draft_model.state_dict()
                new_state_dict, removed_keys = remove_embedding_from_state_dict(
                    original_state_dict, args.embedding_keys
                )
                
                # ä¿å­˜
                os.makedirs(args.output_dir, exist_ok=True)
                model_path = os.path.join(args.output_dir, "pytorch_model.bin")
                torch.save(new_state_dict, model_path)
                print(f"âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜: {model_path}")
                
                # å¤åˆ¶å…¶ä»–æ–‡ä»¶
                copy_other_files(args.input_dir, args.output_dir)
        
        print(f"\nâœ¨ å¤„ç†å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()