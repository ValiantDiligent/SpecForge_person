#!/usr/bin/env python3
"""
æµ‹è¯•å­—ç¬¦ä¸²åœ¨ Qwen3 ä¸­çš„ tokenization
"""

from transformers import AutoTokenizer

def test_tokenization():
    # ä½¿ç”¨ Qwen3 tokenizer
    model_name = "Qwen/Qwen3-0.6B"  # æˆ–è€…ä½ æœ¬åœ°çš„ Qwen3 æ¨¡å‹è·¯å¾„
    print(f"åŠ è½½ tokenizer: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    except Exception as e:
        print(f"åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹: {e}")
        # å¤‡ç”¨æ¨¡å‹
        model_name = "Qwen/Qwen3-0.6B"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # è¦æµ‹è¯•çš„å­—ç¬¦ä¸²
    test_string = '<think>\n\n</think>\n\nå¦<|im_end|>'
    
    print("=" * 60)
    print(f"æµ‹è¯•å­—ç¬¦ä¸²: {repr(test_string)}")
    print(f"å­—ç¬¦ä¸²é•¿åº¦: {len(test_string)} ä¸ªå­—ç¬¦")
    print("=" * 60)
    
    # è¿›è¡Œ tokenizationï¼ˆä¸æ·»åŠ ç‰¹æ®Š tokenï¼‰
    input_ids = tokenizer.encode(test_string, add_special_tokens=False)
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
    print(f"\nğŸ“Š Token ç»Ÿè®¡:")
    print(f"Token æ•°é‡: {len(input_ids)} ä¸ª")
    print(f"Token è¯¦æƒ…:")
    
    for i, (token, token_id) in enumerate(zip(tokens, input_ids)):
        # æ˜¾ç¤ºå¯è¯»çš„ token è¡¨ç¤º
        display_token = token.replace('\n', '\\n').replace(' ', 'â–')
        print(f"  {i+1:2d}. '{display_token}' (ID: {token_id})")
    
    # éªŒè¯è§£ç 
    decoded = tokenizer.decode(input_ids)
    print(f"\nğŸ” è§£ç éªŒè¯:")
    print(f"è§£ç ç»“æœ: {repr(decoded)}")
    print(f"è§£ç åŒ¹é…: {'âœ…' if decoded == test_string else 'âŒ'}")
    
    # åˆ†æå„ä¸ªç»„æˆéƒ¨åˆ†
    print(f"\nğŸ”§ ç»„ä»¶åˆ†æ:")
    components = [
        '<think>',  # å¼€å§‹æ ‡ç­¾
        '\n\n',     # åŒæ¢è¡Œ
        '</think>', # ç»“æŸæ ‡ç­¾
        '\n\n',       # å•æ¢è¡Œ
        'å¦',       # ä¸­æ–‡å­—ç¬¦
        '<|im_end|>',        # ç»“æŸ
        '<|im_end|>\n' 
    ]
    
    total_component_tokens = 0
    for comp in components:
        comp_ids = tokenizer.encode(comp, add_special_tokens=False)
        comp_tokens = tokenizer.convert_ids_to_tokens(comp_ids)
        display_comp = comp.replace('\n', '\\n')
        display_tokens = [t.replace('\n', '\\n').replace(' ', 'â–') for t in comp_tokens]
        print(f"  '{display_comp}' -> {len(comp_ids)} tokens: {display_tokens}")
        total_component_tokens += len(comp_ids)
    
    print(f"\nç»„ä»¶æ€» token æ•°: {total_component_tokens}")
    print(f"æ•´ä½“ token æ•°: {len(input_ids)}")
    print(f"å·®å¼‚: {len(input_ids) - total_component_tokens} (å¯èƒ½ç”±äºä¸Šä¸‹æ–‡æ•ˆåº”)")
    
    # æµ‹è¯•ä¸åŒçš„å˜ä½“
    print(f"\nğŸ§ª å˜ä½“æµ‹è¯•:")
    variants = [
        '<think>\n\n</think>\n\nå¦',  # æ— å¼•å·ç‰ˆæœ¬
        '<think></think>å¦',        # æ— æ¢è¡Œç‰ˆæœ¬
        '<think>\n\n</think>\n\nå¦<|im_end|>',  # åŒ…å«ç»“æŸæ ‡è®°
        '<think>\n\n</think>\n\n    ',                      # åªæœ‰ä¸­æ–‡å­—ç¬¦
        '<think>\n\n</think>\n\n',     # åªæœ‰æ ‡ç­¾éƒ¨åˆ†
    ]
    
    for variant in variants:
        var_ids = tokenizer.encode(variant, add_special_tokens=False)
        display_variant = variant.replace('\n', '\\n')
        print(f"  '{display_variant}' -> {len(var_ids)} tokens")

if __name__ == "__main__":
    test_tokenization()