#!/usr/bin/env python3
"""
æµ‹è¯•æŠ•æœºé‡‡æ ·æœºåˆ¶
éªŒè¯ draft model å’Œ target model çš„äº¤äº’æµç¨‹
"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np

class SpeculativeSamplingDemo:
    def __init__(self, draft_model_name, target_model_name):
        """
        åˆå§‹åŒ–æŠ•æœºé‡‡æ ·æ¼”ç¤º
        
        Args:
            draft_model_name: è‰ç¨¿æ¨¡å‹åç§°ï¼ˆå°æ¨¡å‹ï¼‰
            target_model_name: ç›®æ ‡æ¨¡å‹åç§°ï¼ˆå¤§æ¨¡å‹ï¼‰
        """
        print("åŠ è½½æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(target_model_name)
        
        # æ·»åŠ  pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        try:
            self.draft_model = AutoModelForCausalLM.from_pretrained(
                draft_model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            self.target_model = AutoModelForCausalLM.from_pretrained(
                target_model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨ç›¸åŒæ¨¡å‹è¿›è¡Œæ¼”ç¤º...")
            self.draft_model = AutoModelForCausalLM.from_pretrained(
                target_model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            self.target_model = self.draft_model
        
        print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    def generate_draft_candidates(self, input_ids, num_candidates=3, temperature=0.8):
        """
        ä½¿ç”¨ draft model ç”Ÿæˆå€™é€‰ tokens
        
        Args:
            input_ids: è¾“å…¥åºåˆ—
            num_candidates: å€™é€‰ token æ•°é‡
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            candidate_ids: å€™é€‰ token IDs
            draft_probs: draft model çš„æ¦‚ç‡åˆ†å¸ƒ
        """
        print(f"\nğŸ¯ Draft Model ç”Ÿæˆ {num_candidates} ä¸ªå€™é€‰ tokens...")
        
        with torch.no_grad():
            candidate_ids = []
            draft_probs = []
            current_input = input_ids.clone()
            
            for i in range(num_candidates):
                # Draft model å‰å‘ä¼ æ’­
                outputs = self.draft_model(current_input)
                logits = outputs.logits[0, -1, :]  # æœ€åä¸€ä¸ªä½ç½®çš„ logits
                
                # åº”ç”¨æ¸©åº¦
                logits = logits / temperature
                probs = F.softmax(logits, dim=-1)
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ª token
                next_token = torch.multinomial(probs, 1)
                candidate_ids.append(next_token.item())
                draft_probs.append(probs[next_token].item())
                
                # æ›´æ–°è¾“å…¥åºåˆ—
                current_input = torch.cat([current_input, next_token.unsqueeze(0)], dim=1)
                
                # è§£ç å¹¶æ˜¾ç¤º
                token_text = self.tokenizer.decode(next_token, skip_special_tokens=True)
                print(f"  å€™é€‰ {i+1}: '{token_text}' (ID: {next_token.item()}, æ¦‚ç‡: {probs[next_token].item():.4f})")
        
        return candidate_ids, draft_probs
    
    def verify_with_target(self, input_ids, candidate_ids, draft_probs, temperature=0.8):
        """
        ä½¿ç”¨ target model éªŒè¯å€™é€‰ tokens
        
        Args:
            input_ids: åŸå§‹è¾“å…¥åºåˆ—
            candidate_ids: draft model ç”Ÿæˆçš„å€™é€‰ tokens
            draft_probs: draft model çš„æ¦‚ç‡
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            accepted_tokens: è¢«æ¥å—çš„ tokens
            rejection_point: æ‹’ç»ç‚¹ä½ç½®ï¼ˆ-1 è¡¨ç¤ºå…¨éƒ¨æ¥å—ï¼‰
        """
        print(f"\nğŸ” Target Model éªŒè¯å€™é€‰ tokens...")
        
        with torch.no_grad():
            accepted_tokens = []
            current_input = input_ids.clone()
            
            for i, candidate_id in enumerate(candidate_ids):
                # Target model å‰å‘ä¼ æ’­
                outputs = self.target_model(current_input)
                logits = outputs.logits[0, -1, :]  # æœ€åä¸€ä¸ªä½ç½®çš„ logits
                
                # åº”ç”¨æ¸©åº¦
                logits = logits / temperature
                target_probs = F.softmax(logits, dim=-1)
                
                # è·å–å€™é€‰ token åœ¨ target model ä¸­çš„æ¦‚ç‡
                target_prob = target_probs[candidate_id].item()
                draft_prob = draft_probs[i]
                
                # æŠ•æœºé‡‡æ ·çš„æ¥å—æ¦‚ç‡
                accept_prob = min(1.0, target_prob / draft_prob)
                
                # éšæœºå†³å®šæ˜¯å¦æ¥å—
                if torch.rand(1).item() < accept_prob:
                    # æ¥å—è¿™ä¸ª token
                    accepted_tokens.append(candidate_id)
                    current_input = torch.cat([current_input, torch.tensor([[candidate_id]])], dim=1)
                    
                    token_text = self.tokenizer.decode([candidate_id], skip_special_tokens=True)
                    print(f"  âœ… æ¥å— token {i+1}: '{token_text}' (æ¥å—æ¦‚ç‡: {accept_prob:.4f})")
                    print(f"     Targetæ¦‚ç‡: {target_prob:.4f}, Draftæ¦‚ç‡: {draft_prob:.4f}")
                else:
                    # æ‹’ç»è¿™ä¸ª token
                    token_text = self.tokenizer.decode([candidate_id], skip_special_tokens=True)
                    print(f"  âŒ æ‹’ç» token {i+1}: '{token_text}' (æ¥å—æ¦‚ç‡: {accept_prob:.4f})")
                    print(f"     Targetæ¦‚ç‡: {target_prob:.4f}, Draftæ¦‚ç‡: {draft_prob:.4f}")
                    
                    # ä» target model é‡æ–°é‡‡æ ·
                    adjusted_probs = torch.clamp(target_probs - draft_probs.unsqueeze(0) * target_probs, min=0)
                    if adjusted_probs.sum() > 0:
                        adjusted_probs = adjusted_probs / adjusted_probs.sum()
                        new_token = torch.multinomial(adjusted_probs, 1)
                        accepted_tokens.append(new_token.item())
                        
                        new_token_text = self.tokenizer.decode(new_token, skip_special_tokens=True)
                        print(f"  ğŸ”„ é‡æ–°é‡‡æ ·: '{new_token_text}' (ID: {new_token.item()})")
                    
                    return accepted_tokens, i  # è¿”å›æ‹’ç»ç‚¹
            
            # å¦‚æœæ‰€æœ‰å€™é€‰éƒ½è¢«æ¥å—ï¼Œtarget model ä¼šç”Ÿæˆé¢å¤–çš„ token
            print(f"  ğŸ‰ æ‰€æœ‰ {len(candidate_ids)} ä¸ªå€™é€‰éƒ½è¢«æ¥å—ï¼")
            
            # Target model ç”Ÿæˆç¬¬ 5 ä¸ª tokenï¼ˆé¢å¤–å¥–åŠ±ï¼‰
            outputs = self.target_model(current_input)
            logits = outputs.logits[0, -1, :]
            logits = logits / temperature
            probs = F.softmax(logits, dim=-1)
            bonus_token = torch.multinomial(probs, 1)
            accepted_tokens.append(bonus_token.item())
            
            bonus_text = self.tokenizer.decode(bonus_token, skip_special_tokens=True)
            print(f"  ğŸ å¥–åŠ± token: '{bonus_text}' (ID: {bonus_token.item()})")
            
            return accepted_tokens, -1  # -1 è¡¨ç¤ºå…¨éƒ¨æ¥å—
    
    def demo_speculative_sampling(self, prompt, num_steps=3, temperature=0.8):
        """
        æ¼”ç¤ºå®Œæ•´çš„æŠ•æœºé‡‡æ ·æµç¨‹
        
        Args:
            prompt: è¾“å…¥æç¤º
            num_steps: draft model çš„æ­¥æ•°
            temperature: é‡‡æ ·æ¸©åº¦
        """
        print("=" * 80)
        print("ğŸš€ æŠ•æœºé‡‡æ ·æ¼”ç¤º")
        print("=" * 80)
        print(f"ğŸ“ è¾“å…¥æç¤º: '{prompt}'")
        print(f"ğŸ² Draft steps: {num_steps}")
        print(f"ğŸŒ¡ï¸  Temperature: {temperature}")
        
        # ç¼–ç è¾“å…¥
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        print(f"ğŸ“Š è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")
        
        # ç¬¬1æ­¥ï¼šTarget model ç”Ÿæˆç¬¬ä¸€ä¸ª token
        print(f"\nğŸ¯ Target Model ç”Ÿæˆç¬¬1ä¸ªtoken...")
        with torch.no_grad():
            outputs = self.target_model(input_ids)
            logits = outputs.logits[0, -1, :]
            logits = logits / temperature
            probs = F.softmax(logits, dim=-1)
            first_token = torch.multinomial(probs, 1)
            
            first_text = self.tokenizer.decode(first_token, skip_special_tokens=True)
            print(f"  ç¬¬1ä¸ªtoken: '{first_text}' (ID: {first_token.item()})")
            
            # æ›´æ–°è¾“å…¥åºåˆ—
            input_ids = torch.cat([input_ids, first_token.unsqueeze(0)], dim=1)
        
        # ç¬¬2æ­¥ï¼šDraft model ç”Ÿæˆå€™é€‰ tokensï¼ˆç¬¬2åˆ°ç¬¬4ä¸ªï¼‰
        candidate_ids, draft_probs = self.generate_draft_candidates(
            input_ids, num_candidates=num_steps, temperature=temperature
        )
        
        # ç¬¬3æ­¥ï¼šTarget model éªŒè¯å€™é€‰ tokens
        accepted_tokens, rejection_point = self.verify_with_target(
            input_ids, candidate_ids, draft_probs, temperature=temperature
        )
        
        # ç»“æœæ€»ç»“
        print(f"\nğŸ“‹ æ€»ç»“:")
        print(f"  å€™é€‰ tokens: {len(candidate_ids)}")
        print(f"  æ¥å— tokens: {len(accepted_tokens)}")
        
        if rejection_point == -1:
            print(f"  ğŸ‰ æ‰€æœ‰å€™é€‰éƒ½è¢«æ¥å—ï¼Œè¿˜è·å¾—äº†1ä¸ªå¥–åŠ±tokenï¼")
            print(f"  ğŸ“ˆ æ€»åŠ é€Ÿ: ç”Ÿæˆäº†{len(accepted_tokens)}ä¸ªtokenï¼Œåªéœ€è¦{2}æ¬¡target modelè°ƒç”¨")
            efficiency = len(accepted_tokens) / 2
        else:
            print(f"  âŒ åœ¨ç¬¬{rejection_point + 1}ä¸ªå€™é€‰å¤„è¢«æ‹’ç»")
            print(f"  ğŸ“ˆ éƒ¨åˆ†åŠ é€Ÿ: ç”Ÿæˆäº†{len(accepted_tokens)}ä¸ªtokenï¼Œéœ€è¦{rejection_point + 2}æ¬¡target modelè°ƒç”¨")
            efficiency = len(accepted_tokens) / (rejection_point + 2)
        
        print(f"  âš¡ æ•ˆç‡æå‡: {efficiency:.2f}x")
        
        # æ˜¾ç¤ºæœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬
        final_tokens = torch.cat([input_ids, torch.tensor([accepted_tokens])], dim=1)
        final_text = self.tokenizer.decode(final_tokens[0], skip_special_tokens=True)
        print(f"  ğŸ“„ æœ€ç»ˆæ–‡æœ¬: '{final_text}'")
        
        return accepted_tokens, rejection_point

def main():
    # ç”±äºéœ€è¦ä¸¤ä¸ªä¸åŒå¤§å°çš„æ¨¡å‹ï¼Œè¿™é‡Œç”¨ä¸€ä¸ªæ¨¡å‹æ¼”ç¤ºåŸç†
    try:
        demo = SpeculativeSamplingDemo(
            draft_model_name="Qwen/Qwen2.5-0.5B-Instruct",    # å°æ¨¡å‹ä½œä¸ºdraft
            target_model_name="Qwen/Qwen2.5-1.5B-Instruct"    # å¤§æ¨¡å‹ä½œä¸ºtarget
        )
    except:
        print("ä½¿ç”¨æœ¬åœ°å¯ç”¨çš„æ¨¡å‹è¿›è¡Œæ¼”ç¤º...")
        demo = SpeculativeSamplingDemo(
            draft_model_name="Qwen/Qwen2.5-1.5B-Instruct",
            target_model_name="Qwen/Qwen2.5-1.5B-Instruct"
        )
    
    # æµ‹è¯•ä¸åŒçš„prompt
    test_prompts = [
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "äººå·¥æ™ºèƒ½çš„å‘å±•",
        "Pythonç¼–ç¨‹è¯­è¨€",
    ]
    
    for prompt in test_prompts:
        demo.demo_speculative_sampling(prompt, num_steps=3, temperature=0.8)
        print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    main()