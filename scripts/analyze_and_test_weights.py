#!/usr/bin/env python3
"""
åˆ†æå’Œæµ‹è¯•draft modelæƒé‡çš„è„šæœ¬
1. æŸ¥çœ‹æƒé‡ç»“æ„
2. ç§»é™¤embeddingæƒé‡å¹¶ä¿å­˜
3. æµ‹è¯•åŠ è½½åŠŸèƒ½
"""

import os
import sys
import torch
from safetensors import safe_open
from safetensors.torch import save_file
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def analyze_weights_structure(checkpoint_path):
    """åˆ†ææƒé‡æ–‡ä»¶ç»“æ„"""
    print(f"\n=== åˆ†ææƒé‡æ–‡ä»¶: {checkpoint_path} ===")
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return None
    
    # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
    if checkpoint_path.endswith('.safetensors'):
        print("ğŸ“ æ–‡ä»¶æ ¼å¼: SafeTensors")
        return analyze_safetensors(checkpoint_path)
    elif checkpoint_path.endswith('.bin') or checkpoint_path.endswith('.pt') or checkpoint_path.endswith('.pth'):
        print("ğŸ“ æ–‡ä»¶æ ¼å¼: PyTorch")
        return analyze_pytorch_weights(checkpoint_path)
    else:
        print("â“ æœªçŸ¥æ–‡ä»¶æ ¼å¼")
        return None

def analyze_safetensors(file_path):
    """åˆ†æsafetensorsæ–‡ä»¶"""
    try:
        weights = {}
        with safe_open(file_path, framework="pt", device="cpu") as f:
            print(f"ğŸ” æ€»å…±æœ‰ {len(f.keys())} ä¸ªæƒé‡å¼ é‡")
            print("\nğŸ“Š æƒé‡ç»“æ„åˆ†æ:")
            
            embedding_keys = []
            non_embedding_keys = []
            
            for key in f.keys():
                tensor = f.get_tensor(key)
                weights[key] = tensor
                
                # åˆ†ç±»æƒé‡
                if any(embed_keyword in key.lower() for embed_keyword in ['embed', 'embedding', 'token']):
                    embedding_keys.append(key)
                else:
                    non_embedding_keys.append(key)
                
                print(f"  {key}: {tensor.shape} ({tensor.dtype})")
            
            print(f"\nğŸ¯ Embeddingç›¸å…³æƒé‡ ({len(embedding_keys)} ä¸ª):")
            for key in embedding_keys:
                tensor = weights[key]
                print(f"  âœ“ {key}: {tensor.shape} ({tensor.dtype})")
            
            print(f"\nğŸ”§ éEmbeddingæƒé‡ ({len(non_embedding_keys)} ä¸ª):")
            for key in non_embedding_keys[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                tensor = weights[key]
                print(f"  â€¢ {key}: {tensor.shape} ({tensor.dtype})")
            if len(non_embedding_keys) > 10:
                print(f"  ... è¿˜æœ‰ {len(non_embedding_keys) - 10} ä¸ªæƒé‡")
        
        return weights, embedding_keys, non_embedding_keys
        
    except Exception as e:
        print(f"âŒ è¯»å–safetensorsæ–‡ä»¶å¤±è´¥: {e}")
        return None

def analyze_pytorch_weights(file_path):
    """åˆ†æPyTorchæƒé‡æ–‡ä»¶"""
    try:
        weights = torch.load(file_path, map_location='cpu')
        print(f"ğŸ” æ€»å…±æœ‰ {len(weights)} ä¸ªæƒé‡å¼ é‡")
        print("\nğŸ“Š æƒé‡ç»“æ„åˆ†æ:")
        
        embedding_keys = []
        non_embedding_keys = []
        
        for key, tensor in weights.items():
            # åˆ†ç±»æƒé‡
            if any(embed_keyword in key.lower() for embed_keyword in ['embed', 'embedding', 'token']):
                embedding_keys.append(key)
            else:
                non_embedding_keys.append(key)
            
            print(f"  {key}: {tensor.shape} ({tensor.dtype})")
        
        print(f"\nğŸ¯ Embeddingç›¸å…³æƒé‡ ({len(embedding_keys)} ä¸ª):")
        for key in embedding_keys:
            tensor = weights[key]
            print(f"  âœ“ {key}: {tensor.shape} ({tensor.dtype})")
        
        print(f"\nğŸ”§ éEmbeddingæƒé‡ ({len(non_embedding_keys)} ä¸ª):")
        for key in non_embedding_keys[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            tensor = weights[key]
            print(f"  â€¢ {key}: {tensor.shape} ({tensor.dtype})")
        if len(non_embedding_keys) > 10:
            print(f"  ... è¿˜æœ‰ {len(non_embedding_keys) - 10} ä¸ªæƒé‡")
        
        return weights, embedding_keys, non_embedding_keys
        
    except Exception as e:
        print(f"âŒ è¯»å–PyTorchæ–‡ä»¶å¤±è´¥: {e}")
        return None

def remove_embedding_and_save(weights, embedding_keys, non_embedding_keys, original_path):
    """ç§»é™¤embeddingæƒé‡å¹¶ä¿å­˜"""
    print(f"\n=== ç§»é™¤Embeddingæƒé‡å¹¶ä¿å­˜ ===")
    
    # åˆ›å»ºä¸åŒ…å«embeddingçš„æƒé‡å­—å…¸
    weights_no_embed = {}
    for key in non_embedding_keys:
        if isinstance(weights, dict):
            weights_no_embed[key] = weights[key]
        else:
            # å¦‚æœæ˜¯safetensorsï¼Œéœ€è¦é‡æ–°è¯»å–
            with safe_open(original_path, framework="pt", device="cpu") as f:
                weights_no_embed[key] = f.get_tensor(key)
    
    print(f"âœ‚ï¸ ç§»é™¤äº† {len(embedding_keys)} ä¸ªembeddingæƒé‡")
    print(f"ğŸ’¾ ä¿ç•™äº† {len(weights_no_embed)} ä¸ªéembeddingæƒé‡")
    
    # ä¿å­˜æ–°çš„æƒé‡æ–‡ä»¶
    output_path = original_path.replace('.safetensors', '_no_embed.safetensors')
    if not output_path.endswith('_no_embed.safetensors'):
        output_path = original_path.replace('.bin', '_no_embed.safetensors')
        output_path = output_path.replace('.pt', '_no_embed.safetensors')
        output_path = output_path.replace('.pth', '_no_embed.safetensors')
    
    try:
        save_file(weights_no_embed, output_path)
        print(f"âœ… æˆåŠŸä¿å­˜åˆ°: {output_path}")
        return output_path
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return None

def test_loading_with_from_pretrained(no_embed_path):
    """æµ‹è¯•ä½¿ç”¨from_pretrainedåŠ è½½æ— embeddingæƒé‡"""
    print(f"\n=== æµ‹è¯•åŠ è½½æ— Embeddingæƒé‡ ===")
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from specforge.modeling.auto import AutoEagle3DraftModel
        from specforge.modeling.auto import AutoDraftModelConfig
        
        # åŠ è½½é…ç½®
        config_path = "configs/qwen3-8b-eagle3.json"
        if not os.path.exists(config_path):
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
        
        print(f"ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        draft_model_config = AutoDraftModelConfig.from_file(config_path)
        
        # å°è¯•åŠ è½½æ¨¡å‹ï¼ˆæ¨¡æ‹Ÿtrain_eagle3_online.pyä¸­çš„ä»£ç ï¼‰
        print(f"ğŸ”„ å°è¯•åŠ è½½æ¨¡å‹: {no_embed_path}")
        
        # è¿™é‡Œæ¨¡æ‹Ÿç¬¬155-159è¡Œçš„ä»£ç 
        draft_model = (
            AutoEagle3DraftModel.from_pretrained(no_embed_path)
            .cuda() if torch.cuda.is_available() else AutoEagle3DraftModel.from_pretrained(no_embed_path)
        )
        
        if torch.cuda.is_available():
            draft_model = draft_model.to(torch.bfloat16)
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in draft_model.parameters()):,}")
        
        # æ£€æŸ¥embeddingå±‚çŠ¶æ€
        print("\nğŸ” æ£€æŸ¥embeddingå±‚çŠ¶æ€:")
        for name, module in draft_model.named_modules():
            if 'embed' in name.lower():
                print(f"  {name}: {type(module)}")
                if hasattr(module, 'weight'):
                    print(f"    æƒé‡å½¢çŠ¶: {module.weight.shape}")
                    print(f"    æƒé‡è®¾å¤‡: {module.weight.device}")
                    print(f"    æƒé‡ç±»å‹: {module.weight.dtype}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Draft Modelæƒé‡åˆ†æå’Œæµ‹è¯•å·¥å…·")
    
    # æƒé‡æ–‡ä»¶è·¯å¾„
    checkpoint_path = "/Users/zhanghuaxiang/zhx/go_learner/src/SpecForge-1/cache/dataset/Qwen3-8B-eagle3/epoch_9"
    
    # 1. åˆ†ææƒé‡ç»“æ„
    result = analyze_weights_structure(checkpoint_path)
    if result is None:
        print("âŒ æƒé‡åˆ†æå¤±è´¥ï¼Œé€€å‡º")
        return
    
    weights, embedding_keys, non_embedding_keys = result
    
    # 2. ç§»é™¤embeddingå¹¶ä¿å­˜
    no_embed_path = remove_embedding_and_save(weights, embedding_keys, non_embedding_keys, checkpoint_path)
    if no_embed_path is None:
        print("âŒ ç§»é™¤embeddingå¤±è´¥ï¼Œé€€å‡º")
        return
    
    # 3. æµ‹è¯•åŠ è½½åŠŸèƒ½
    success = test_loading_with_from_pretrained(no_embed_path)
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… è¯æ˜ï¼šå³ä½¿é¢„è®­ç»ƒæƒé‡æ²¡æœ‰embeddingï¼Œæ¨¡å‹ä¹Ÿèƒ½æ­£å¸¸åŠ è½½")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()