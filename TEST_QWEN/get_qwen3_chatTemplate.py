from transformers import AutoTokenizer
import json
import re

# åˆå§‹åŒ–åˆ†è¯å™¨ - ä½¿ç”¨ Qwen3
model_name = "Qwen/Qwen3-8B"  # æˆ–è€… "Qwen/Qwen2.5-7B-Instruct"
print(f"Loading tokenizer for {model_name}...")

try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
except Exception as e:
    print(f"Failed to load {model_name}, trying backup model...")
    model_name = "Qwen/Qwen2.5-7B-Instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

print(f"Successfully loaded: {model_name}")

def display_with_newlines(text, title):
    """æ˜¾ç¤ºæ–‡æœ¬ï¼Œä¿ç•™æ¢è¡Œç¬¦çš„å¯è§æ€§"""
    print(f"\n{title}:")
    print("=" * 60)
    print("Raw representation (visible \\n):")
    print(repr(text))
    print("\nFormatted text:")
    print(text)
    print("=" * 60)

def analyze_tokens(text, title):
    """åˆ†ætokenï¼ŒåŒ…æ‹¬ç‰¹æ®Štoken"""
    print(f"\n{title} Token Analysis:")
    print("-" * 40)
    
    # ç¼–ç ï¼ˆä¿ç•™ç‰¹æ®Štokenï¼‰
    tokens_with_special = tokenizer.encode(text, add_special_tokens=False)
    tokens_regular = tokenizer.encode(text, add_special_tokens=True)
    
    print(f"Token count (no special): {len(tokens_with_special)}")
    print(f"Token count (with special): {len(tokens_regular)}")
    print(f"First 15 token IDs: {tokens_with_special[:15]}")
    
    # æ£€æŸ¥æ€è€ƒç›¸å…³çš„ç‰¹æ®Štoken
    think_start_ids = [151667, 151664]  # å¯èƒ½çš„<think> token IDs
    think_end_ids = [151668, 151665]    # å¯èƒ½çš„</think> token IDs
    
    has_think_start = any(tid in tokens_with_special for tid in think_start_ids)
    has_think_end = any(tid in tokens_with_special for tid in think_end_ids)
    
    print(f"Contains <think> tokens: {has_think_start}")
    print(f"Contains </think> tokens: {has_think_end}")
    
    # è§£ç éªŒè¯
    decoded_with_special = tokenizer.decode(tokens_with_special, skip_special_tokens=False)
    decoded_clean = tokenizer.decode(tokens_with_special, skip_special_tokens=True)
    
    print("Decoded (with special tokens):")
    print(repr(decoded_with_special))
    
    # æ£€æŸ¥ç‰¹æ®Šæ ‡ç­¾
    special_tags = re.findall(r'<[^>]+>', decoded_with_special)
    if special_tags:
        print(f"Special tags found: {list(set(special_tags))}")
    else:
        print("No special tags found")
    
    return tokens_with_special, decoded_with_special

def test_chat_template(messages, description, test_thinking=True):
    """æµ‹è¯•èŠå¤©æ¨¡æ¿ï¼ŒåŒ…æ‹¬æ€è€ƒå’Œéæ€è€ƒæ¨¡å¼"""
    print(f"\n\n{'#' * 80}")
    print(f"# {description}")
    print(f"{'#' * 80}")
    
    print("Input Messages:")
    print(json.dumps(messages, indent=2, ensure_ascii=False))
    
    # æµ‹è¯•ä¸åŒçš„æ€è€ƒæ¨¡å¼
    thinking_modes = []
    if test_thinking:
        thinking_modes = [
            {"enable_thinking": False, "name": "éæ€è€ƒæ¨¡å¼"},
            {"enable_thinking": True, "name": "æ€è€ƒæ¨¡å¼"}
        ]
    else:
        thinking_modes = [{"enable_thinking": False, "name": "æ ‡å‡†æ¨¡å¼"}]
    
    for mode in thinking_modes:
        print(f"\n{'-' * 50}")
        print(f"æ¨¡å¼: {mode['name']}")
        print(f"{'-' * 50}")
        
        try:
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            if 'enable_thinking' in mode:
                formatted_prompt = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=mode['enable_thinking']
                )
            else:
                formatted_prompt = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
            
            display_with_newlines(formatted_prompt, f"Chat Template Output ({mode['name']})")
            analyze_tokens(formatted_prompt, mode['name'])
            
        except Exception as e:
            print(f"Error in {mode['name']}: {e}")
            # å°è¯•ä¸å¸¦enable_thinkingå‚æ•°
            try:
                formatted_prompt = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                display_with_newlines(formatted_prompt, f"Fallback Template Output")
                analyze_tokens(formatted_prompt, "Fallback")
            except Exception as e2:
                print(f"Fallback also failed: {e2}")

# æ˜¾ç¤ºèŠå¤©æ¨¡æ¿
print(f"\n{'=' * 80}")
print(f"Chat Template for {model_name}:")
print(f"{'=' * 80}")
if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
    print(tokenizer.chat_template)
else:
    print("No chat template found")

# æµ‹è¯•ç”¨ä¾‹1: ç®€å•é—®å€™
test_chat_template(
    [{"role": "user", "content": "ä½ å¥½ï¼"}],
    "æµ‹è¯•1: ç®€å•ç”¨æˆ·é—®å€™"
)

# æµ‹è¯•ç”¨ä¾‹2: åŒ…å«æ¢è¡Œç¬¦çš„å¤æ‚å†…å®¹
test_chat_template(
    [{"role": "user", "content": "è¯·è§£é‡Šæœºå™¨å­¦ä¹ ï¼š\n1. ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\n2. ä¸»è¦åº”ç”¨é¢†åŸŸ\n3. æœªæ¥å‘å±•è¶‹åŠ¿\n\nè¯·è¯¦ç»†å›ç­”æ¯ä¸ªé—®é¢˜ã€‚"}],
    "æµ‹è¯•2: åŒ…å«å¤šä¸ªæ¢è¡Œç¬¦çš„å¤æ‚é—®é¢˜"
)

# æµ‹è¯•ç”¨ä¾‹3: å¸¦ç³»ç»Ÿæç¤ºè¯çš„ç®€å•å¯¹è¯
test_chat_template(
    [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ä¿æŒç¤¼è²Œå’Œå‡†ç¡®ã€‚"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"}
    ],
    "æµ‹è¯•3: å¸¦ç³»ç»Ÿæç¤ºè¯çš„å¯¹è¯"
)

# æµ‹è¯•ç”¨ä¾‹4: å¤æ‚ç³»ç»Ÿæç¤ºè¯
test_chat_template(
    [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹ä¸“å®¶ã€‚\nè§„åˆ™ï¼š\n1. æä¾›è¯¦ç»†çš„ä»£ç è§£é‡Š\n2. åŒ…å«å®é™…å¯è¿è¡Œçš„ç¤ºä¾‹\n3. è¯´æ˜æœ€ä½³å®è·µ\n4. æŒ‡å‡ºå¸¸è§é”™è¯¯\n\nè¯·å§‹ç»ˆéµå¾ªè¿™äº›è§„åˆ™ã€‚"},
        {"role": "user", "content": "å¦‚ä½•å®ç°ä¸€ä¸ªé«˜æ•ˆçš„å¿«é€Ÿæ’åºç®—æ³•ï¼Ÿ"}
    ],
    "æµ‹è¯•4: å¤æ‚å¤šè¡Œç³»ç»Ÿæç¤ºè¯"
)

# æµ‹è¯•ç”¨ä¾‹5: å¤šè½®å¯¹è¯
test_chat_template(
    [
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"},
        {"role": "assistant", "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚"},
        {"role": "user", "content": "å®ƒä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\nè¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å¯¹æ¯”ï¼š\n- æ•°æ®éœ€æ±‚\n- è®¡ç®—å¤æ‚åº¦\n- åº”ç”¨åœºæ™¯"}
    ],
    "æµ‹è¯•5: å¤šè½®å¯¹è¯ä¸æ¢è¡Œç¬¦"
)

# æµ‹è¯•ç”¨ä¾‹6: å¤æ‚ç³»ç»Ÿæç¤ºè¯ + å¤šè½®å¯¹è¯
test_chat_template(
    [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå“²å­¦æ•™æˆã€‚\nè¯·æ³¨æ„ï¼š\n- æ·±å…¥åˆ†ææ¯ä¸ªæ¦‚å¿µ\n- æä¾›å†å²èƒŒæ™¯\n- ä¸¾å‡ºå…·ä½“ä¾‹å­\n- å¼•ç”¨ç›¸å…³å“²å­¦å®¶çš„è§‚ç‚¹"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯è‡ªç”±æ„å¿—ï¼Ÿ"},
        {"role": "assistant", "content": "è‡ªç”±æ„å¿—æ˜¯å“²å­¦ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µ...\n\nä»å†å²è§’åº¦çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è¿½æº¯åˆ°å¤å¸Œè…Šæ—¶æœŸã€‚"},
        {"role": "user", "content": "å¦‚æœæ²¡æœ‰è‡ªç”±æ„å¿—ï¼Œé“å¾·è´£ä»»è¿˜å­˜åœ¨å—ï¼Ÿ\nè¿™ä¸ªé—®é¢˜å¾ˆå¤æ‚ï¼Œè¯·æ·±å…¥æ€è€ƒã€‚"}
    ],
    "æµ‹è¯•6: å¤æ‚ç³»ç»Ÿæç¤ºè¯ + å¤šè½®å¯¹è¯"
)

# æµ‹è¯•ç”¨ä¾‹7: éœ€è¦æ·±åº¦æ€è€ƒçš„æ•°å­¦é—®é¢˜ï¼ˆä¸“é—¨æµ‹è¯•æ€è€ƒæ¨¡å¼ï¼‰
test_chat_template(
    [
        {"role": "user", "content": "è¯·è§£å†³è¿™ä¸ªå¤æ‚çš„æ•°å­¦é—®é¢˜ï¼š\n\nä¸€ä¸ªæ­£å…«é¢ä½“çš„ä½“ç§¯å…¬å¼æ˜¯ä»€ä¹ˆï¼Ÿ\nå¦‚æœè¾¹é•¿ä¸ºaï¼Œè¯·ï¼š\n1. æ¨å¯¼ä½“ç§¯å…¬å¼\n2. è®¡ç®—å½“a=6æ—¶çš„ä½“ç§¯\n3. è¯´æ˜æ¨å¯¼è¿‡ç¨‹\n\nè¿™éœ€è¦ä»”ç»†æ€è€ƒå‡ ä½•å…³ç³»ã€‚"}
    ],
    "æµ‹è¯•7: å¤æ‚æ•°å­¦æ¨å¯¼é—®é¢˜ï¼ˆæµ‹è¯•æ€è€ƒæ¨¡å¼ï¼‰"
)

# æµ‹è¯•ç”¨ä¾‹8: ç¼–ç¨‹é—®é¢˜ï¼ˆå®¹æ˜“è§¦å‘æ€è€ƒï¼‰
test_chat_template(
    [
        {"role": "user", "content": "è®¾è®¡ä¸€ä¸ªç®—æ³•æ¥è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š\n\nç»™å®šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ‰¾å‡ºå…¶ä¸­æœ€é•¿çš„å›æ–‡å­ä¸²ã€‚\nè¦æ±‚ï¼š\n- æ—¶é—´å¤æ‚åº¦å°½å¯èƒ½ä½\n- ç©ºé—´å¤æ‚åº¦ä¹Ÿè¦è€ƒè™‘\n- å¤„ç†è¾¹ç•Œæƒ…å†µ\n- æä¾›å®Œæ•´çš„Pythonå®ç°\n\nè¯·è¯¦ç»†åˆ†æä¸åŒç®—æ³•çš„ä¼˜ç¼ºç‚¹ã€‚"}
    ],
    "æµ‹è¯•8: ç®—æ³•è®¾è®¡é—®é¢˜ï¼ˆæµ‹è¯•æ€è€ƒæ¨¡å¼ï¼‰"
)

# æµ‹è¯•ç”¨ä¾‹9: å“²å­¦æ€è¾¨é—®é¢˜
test_chat_template(
    [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ·±åº¦æ€è€ƒè€…ï¼Œæ“…é•¿å“²å­¦åˆ†æã€‚"},
        {"role": "user", "content": "å¦‚æœäººå·¥æ™ºèƒ½è¾¾åˆ°äº†äººç±»çš„æ™ºèƒ½æ°´å¹³ï¼Œ\nç”šè‡³è¶…è¶Šäº†äººç±»ï¼Œ\né‚£ä¹ˆï¼š\n\n1. å®ƒä»¬æ˜¯å¦åº”è¯¥æ‹¥æœ‰æƒåˆ©ï¼Ÿ\n2. äººç±»å¦‚ä½•å®šä¹‰è‡ªå·±çš„ä»·å€¼ï¼Ÿ\n3. è¿™ç§æƒ…å†µä¸‹çš„ä¼¦ç†æ¡†æ¶åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ\n\nè¿™æ˜¯ä¸€ä¸ªéœ€è¦æ·±å…¥æ€è€ƒçš„å¤æ‚é—®é¢˜ã€‚"}
    ],
    "æµ‹è¯•9: å¤æ‚å“²å­¦æ€è¾¨é—®é¢˜ï¼ˆé‡ç‚¹æµ‹è¯•æ€è€ƒæ¨¡å¼ï¼‰"
)

print(f"\n\n{'=' * 80}")
print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
print("=" * 80)
print("è§‚å¯Ÿè¦ç‚¹ï¼š")
print("1. ğŸ’­ æ¢è¡Œç¬¦åœ¨repr()ä¸­æ˜¾ç¤ºä¸º\\nï¼Œåœ¨æ ¼å¼åŒ–æ–‡æœ¬ä¸­æ­£å¸¸æ˜¾ç¤º")
print("2. ğŸ”„ æ€è€ƒæ¨¡å¼vséæ€è€ƒæ¨¡å¼çš„æ¨¡æ¿å·®å¼‚")
print("3. ğŸ·ï¸  ç‰¹æ®Štoken (å¦‚<think>, </think>)çš„æ£€æµ‹")
print("4. ğŸ¤– ç³»ç»Ÿæç¤ºè¯çš„å¤„ç†æ–¹å¼")
print("5. ğŸ’¬ å¤šè½®å¯¹è¯çš„æ¨¡æ¿ç»“æ„")
print("6. ğŸ§  å¤æ‚é—®é¢˜æ˜¯å¦è§¦å‘æ€è€ƒæ¨¡å¼")
print("7. ğŸ“Š Tokenæ•°é‡å’ŒIDçš„åˆ†æ")
print("=" * 80)